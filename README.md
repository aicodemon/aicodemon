<!--
GitHub Profile README
Repo name MUST equal your GitHub username for this to render on your profile.
Replace placeholders like <...> and remove any sections you don’t want.
-->

<div align="left">

# AI / Full-Stack Engineer (Production Systems)

I build **production-grade AI systems**—LLM features, **RAG**, and **agent workflows**—with a focus on **evaluation, observability, and reliable delivery**.

<a href="<YOUR_PORTFOLIO_OR_NOTION_LINK>"><img src="https://img.shields.io/badge/Portfolio-000000?logo=vercel&logoColor=white" /></a>
<a href="mailto:<YOUR_EMAIL>"><img src="https://img.shields.io/badge/Email-000000?logo=gmail&logoColor=white" /></a>
<a href="<YOUR_CALENDLY_OR_CONTACT_LINK>"><img src="https://img.shields.io/badge/Book%2015%20min-000000?logo=googlecalendar&logoColor=white" /></a>
<a href="<YOUR_DISCORD_SERVER_OR_DM_INSTRUCTIONS_LINK>"><img src="https://img.shields.io/badge/Discord-000000?logo=discord&logoColor=white" /></a>

</div>

---

## What I ship
- **RAG and knowledge systems** that reduce hallucinations and improve answer quality with measurable evals  
- **Tool-using agents** with retries/fallbacks/escalation, idempotency, and audit trails  
- **Evaluation + observability** (golden sets, regression suites, tracing, cost/latency KPIs) to keep AI reliable in production  
- **Backend/data foundations** (APIs, Postgres, queues, integrations) that make AI features real and maintainable  

---

## Selected work (best proof)
> Keep this to **3** items. Outcomes matter more than features.

- **<PROJECT A NAME>** — <one-line outcome: quality / cost / latency / reliability>  
  Repo: <LINK> · Demo/Live: <LINK> · Notes/Case Study: <LINK>

- **<PROJECT B NAME>** — <one-line outcome: eval harness / observability / regressions prevented>  
  Repo: <LINK> · Demo: <LINK>

- **<PROJECT C NAME>** — <one-line outcome: shipped feature / ops reduction / integration>  
  Repo: <LINK> · Demo/Live: <LINK>

---

## Mini case study (5 lines)
**Problem:** <e.g., RAG chatbot produced incorrect answers and citations were inconsistent>  
**Constraints:** <latency/cost/privacy/compliance/timeline>  
**Approach:** <hybrid retrieval + reranking + eval baseline + guardrails + telemetry>  
**Result:** <metric 1> · <metric 2> · <metric 3>  
**Why it worked:** measurable baselines + instrumentation + iterative, risk-managed fixes  

---

## How I work (reliability-first)
- Start with a **failure-mode audit** → propose 2–3 options with tradeoffs
- Establish **eval baselines** before tuning (so improvements are measurable)
- Ship in small increments with **staging previews** and rollback-friendly releases
- Provide **handoff-ready docs**: architecture notes, runbooks, and “how to extend” guides

---

## Skills (rich icons)

### AI / LLM Systems
<p align="left">
  <img src="https://img.shields.io/badge/RAG-000000?logo=openai&logoColor=white" />
  <img src="https://img.shields.io/badge/Agents-000000?logo=githubcopilot&logoColor=white" />
  <img src="https://img.shields.io/badge/Evaluation-000000?logo=pytest&logoColor=white" />
  <img src="https://img.shields.io/badge/Observability-000000?logo=opentelemetry&logoColor=white" />
  <img src="https://img.shields.io/badge/Embeddings-000000?logo=pytorch&logoColor=white" />
</p>

### Core Engineering Toolbox (icon grid)
<p align="left">
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg" height="34" alt="Python" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/typescript/typescript-original.svg" height="34" alt="TypeScript" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/javascript/javascript-original.svg" height="34" alt="JavaScript" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/react/react-original.svg" height="34" alt="React" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nextjs/nextjs-original.svg" height="34" alt="Next.js" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nodejs/nodejs-original.svg" height="34" alt="Node.js" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/fastapi/fastapi-original.svg" height="34" alt="FastAPI" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/django/django-plain.svg" height="34" alt="Django" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/postgresql/postgresql-original.svg" height="34" alt="PostgreSQL" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/mysql/mysql-original.svg" height="34" alt="MySQL" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/redis/redis-original.svg" height="34" alt="Redis" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/docker/docker-original.svg" height="34" alt="Docker" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/kubernetes/kubernetes-plain.svg" height="34" alt="Kubernetes" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/amazonwebservices/amazonwebservices-original.svg" height="34" alt="AWS" />
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/googlecloud/googlecloud-original.svg" height="34" alt="GCP" />
</p>

---

## What I’m optimizing for
- **Measurable AI quality:** evals, regression tests, and monitoring
- **Operational excellence:** tracing, error taxonomy, cost/latency control
- **Maintainability:** readable code, clean interfaces, practical architecture
- **Security & privacy by default:** least privilege, safe tool access, data handling discipline

---

## Open to
- Contract / consulting / full-time (remote-friendly)
- AI product engineering, RAG/agents, eval/observability, backend-heavy AI systems

---

## Contact (privacy-friendly)
- Discord: **<YOUR_DISCORD_HANDLE>**
- Email: **<YOUR_EMAIL>**
- Portfolio/Notes: <YOUR_PORTFOLIO_OR_NOTION_LINK>

> If you share your goal + constraints (latency, cost, privacy, timeline), I can propose a plan and evaluation criteria quickly.
